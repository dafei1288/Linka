import requests
import trafilatura
from markdownify import markdownify as md
import json
from dateutil.parser import parse
from datetime import timezone
from typing import Optional
import asyncio
from bs4 import BeautifulSoup
from image_utils.async_image_analysis import AsyncImageAnalysis
from markdownify import MarkdownConverter
import re
from typing import List, Dict, Any
from markdownify import MarkdownConverter, abstract_inline_conversion, chomp
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import asyncio
import os
from typing import Optional
from image_utils.async_image_analysis import AsyncImageAnalysis


class ImageDescMarkdownConverter(MarkdownConverter):
    """
    è‡ªå®šä¹‰Markdownè½¬æ¢å™¨ï¼Œç»§æ‰¿è‡ªMarkdownConverterã€‚
    æä¾›äº†æ›´çµæ´»çš„HTMLåˆ°Markdownè½¬æ¢é€‰é¡¹ï¼Œä¾‹å¦‚ï¼š
    - è½¬æ¢é“¾æ¥ä¸ºç»å¯¹è·¯å¾„ã€‚
    - ç§»é™¤æ‰€æœ‰é“¾æ¥å’Œå›¾ç‰‡ã€‚
    - ç‰¹æ®Šå¤„ç†åŒ…å«colspanæˆ–rowspançš„è¡¨æ ¼ã€‚
    """

    def __init__(self, **kwargs):
        """
        åˆå§‹åŒ–è‡ªå®šä¹‰çš„Markdownè½¬æ¢å™¨ã€‚

        :param current_url: å½“å‰é¡µé¢çš„URLï¼Œç”¨äºå°†ç›¸å¯¹é“¾æ¥å’Œå›¾ç‰‡è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ã€‚
                            å¦‚æœæä¾›äº†æ­¤å‚æ•°ï¼Œå›¾ç‰‡å’Œé“¾æ¥çš„URLå°†è‡ªåŠ¨è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ã€‚
        :param kwargs: å…¶ä»–ä¼ é€’ç»™çˆ¶ç±»MarkdownConverterçš„å‚æ•°ã€‚
                       ä¾‹å¦‚ï¼šstrip (éœ€è¦ç§»é™¤çš„æ ‡ç­¾åˆ—è¡¨), convert (ä»…è½¬æ¢çš„æ ‡ç­¾åˆ—è¡¨), heading_styleç­‰ã€‚
        """
        super().__init__(**kwargs)
        self.current_url = kwargs.get("current_url", None)  # å­˜å‚¨å½“å‰URLï¼Œç”¨äºè·¯å¾„è½¬æ¢
        self.img_desc_map = kwargs.get("img_desc_map", {})  # æ–°å¢ï¼Œä¿å­˜å›¾ç‰‡æè¿°æ˜ å°„

    def convert_img(self, el, text, parent_tags):
        """
        è½¬æ¢<img>æ ‡ç­¾ä¸ºMarkdownæ ¼å¼çš„å›¾ç‰‡ã€‚
        å¦‚æœæä¾›äº†current_urlï¼Œåˆ™å°†å›¾ç‰‡srcè½¬æ¢ä¸ºç»å¯¹è·¯å¾„ã€‚
        ä»…åšåŸºç¡€çš„Markdownå›¾ç‰‡è¯­æ³•è½¬æ¢ï¼Œä¸å†è¿›è¡Œå›¾ç‰‡åˆ†æã€‚

        :param el: BeautifulSoupçš„Tagå¯¹è±¡ï¼Œä»£è¡¨<img>å…ƒç´ ã€‚
        :param text: å›¾ç‰‡çš„æ›¿ä»£æ–‡æœ¬ï¼ˆé€šå¸¸ä¸ºç©ºï¼Œå› ä¸ºaltå±æ€§ä¼šè¢«å•ç‹¬æå–ï¼‰ã€‚
        :param parent_tags: çˆ¶æ ‡ç­¾é›†åˆï¼Œç”¨äºåˆ¤æ–­ä¸Šä¸‹æ–‡ã€‚
        :return: Markdownæ ¼å¼çš„å›¾ç‰‡å­—ç¬¦ä¸²ï¼Œæˆ–è€…åœ¨ç‰¹å®šæƒ…å†µä¸‹è¿”å›altæ–‡æœ¬æˆ–ç©ºå­—ç¬¦ä¸²ã€‚
        """
        alt_text = el.attrs.get("alt", "") or ""
        src_url = el.attrs.get("src", "") or ""
        title_text = el.attrs.get("title", "") or ""

        if (
            "_inline" in parent_tags
            and el.parent.name not in self.options["keep_inline_images_in"]
        ):
            return alt_text

        if not src_url:
            return alt_text

        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if self.current_url:
            src_url = urljoin(self.current_url, src_url)

        # ä¼˜å…ˆä½¿ç”¨AIåˆ†æç»“æœ
        if self.img_desc_map and src_url in self.img_desc_map:
            desc_info = self.img_desc_map[src_url]
            ai_title = desc_info.get("title") or alt_text or "å›¾ç‰‡"
            ai_desc = desc_info.get("description", "")
            md = f"![{ai_title}]({src_url})"
            if ai_desc:
                md += "\n" + "\n".join(f"> {line}" for line in ai_desc.strip().splitlines())
            return md

        if title_text:
            escaped_title = title_text.replace('"', r"\"")
            title_part = f' "{escaped_title}"'
        else:
            title_part = ""
        return f"![{alt_text}]({src_url}{title_part})"

    def _process_table_element(self, element):
        """
        è¾…åŠ©æ–¹æ³•ï¼šå¤„ç†åŒ…å«colspanæˆ–rowspanå±æ€§çš„è¡¨æ ¼å…ƒç´ ï¼ˆtd, thï¼‰ã€‚
        æ­¤æ–¹æ³•ä¼šè§£æä¼ å…¥çš„è¡¨æ ¼å…ƒç´ å­—ç¬¦ä¸²ï¼Œå¹¶ç§»é™¤é™¤äº†'colspan'å’Œ'rowspan'ä¹‹å¤–çš„æ‰€æœ‰å±æ€§ã€‚
        ç›®çš„æ˜¯åœ¨ä¿ç•™è¡¨æ ¼ç»“æ„çš„åŒæ—¶ï¼Œç®€åŒ–HTMLï¼Œä»¥ä¾¿åç»­å¯èƒ½ç”±å…¶ä»–å·¥å…·æˆ–æ‰‹åŠ¨è¿›è¡Œæ›´å¤æ‚çš„Markdownè½¬æ¢ã€‚

        :param element: BeautifulSoupçš„Tagå¯¹è±¡ï¼Œä»£è¡¨ä¸€ä¸ªHTMLè¡¨æ ¼å…ƒç´ ï¼ˆå¦‚<table>, <tr>, <td>, <th>ï¼‰ã€‚
        :return: å¤„ç†åçš„HTMLå…ƒç´ å­—ç¬¦ä¸²ï¼Œä»…ä¿ç•™colspanå’Œrowspanå±æ€§ã€‚
        """
        # ä½¿ç”¨BeautifulSoupè§£æä¼ å…¥çš„å…ƒç´ å­—ç¬¦ä¸²ï¼Œç¡®ä¿æ“ä½œçš„æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„DOMç»“æ„
        soup = BeautifulSoup(str(element), "html.parser")
        # éå†soupä¸­çš„æ‰€æœ‰æ ‡ç­¾
        for tag in soup.find_all(True):
            # å®šä¹‰éœ€è¦ä¿ç•™çš„å±æ€§åˆ—è¡¨
            attrs_to_keep = ["colspan", "rowspan"]
            # æ›´æ–°æ ‡ç­¾çš„å±æ€§å­—å…¸ï¼Œåªä¿ç•™åœ¨attrs_to_keepåˆ—è¡¨ä¸­çš„å±æ€§
            tag.attrs = {
                key: value for key, value in tag.attrs.items() if key in attrs_to_keep
            }
        # è¿”å›å¤„ç†åsoupçš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼
        return str(soup)

    def convert_table(self, el, text, parent_tags):
        """
        è½¬æ¢<table>æ ‡ç­¾ä¸ºMarkdownæ ¼å¼ã€‚
        å¦‚æœè¡¨æ ¼ä¸­çš„<td>æˆ–<th>æ ‡ç­¾åŒ…å«colspanæˆ–rowspanå±æ€§ï¼Œ
        åˆ™è°ƒç”¨_process_table_elementæ–¹æ³•è¿”å›å¤„ç†è¿‡çš„HTMLå­—ç¬¦ä¸²ï¼ˆä¿ç•™ç»“æ„ä½†ç®€åŒ–å±æ€§ï¼‰ï¼Œ
        å¦åˆ™ï¼Œè°ƒç”¨çˆ¶ç±»çš„convert_tableæ–¹æ³•è¿›è¡Œæ ‡å‡†è½¬æ¢ã€‚

        :param el: BeautifulSoupçš„Tagå¯¹è±¡ï¼Œä»£è¡¨<table>å…ƒç´ ã€‚
        :param text: è¡¨æ ¼çš„å†…éƒ¨æ–‡æœ¬å†…å®¹ï¼ˆé€šå¸¸ç”±å­å…ƒç´ çš„è½¬æ¢ç»“æœæ‹¼æ¥è€Œæˆï¼‰ã€‚
        :param parent_tags: çˆ¶æ ‡ç­¾é›†åˆï¼Œç”¨äºåˆ¤æ–­ä¸Šä¸‹æ–‡ã€‚
        :return: Markdownæ ¼å¼çš„è¡¨æ ¼å­—ç¬¦ä¸²ï¼Œæˆ–è€…åœ¨åŒ…å«åˆå¹¶å•å…ƒæ ¼æ—¶è¿”å›å¤„ç†åçš„HTMLå­—ç¬¦ä¸²ã€‚
        """
        # ä½¿ç”¨BeautifulSoupè§£æä¼ å…¥çš„<table>å…ƒç´ å­—ç¬¦ä¸²
        soup = BeautifulSoup(str(el), "html.parser")
        # æ£€æŸ¥è¡¨æ ¼ä¸­æ˜¯å¦å­˜åœ¨ä»»ä½•å¸¦æœ‰colspanæˆ–rowspanå±æ€§çš„<td>æˆ–<th>æ ‡ç­¾
        has_colspan_or_rowspan = any(
            tag.has_attr("colspan") or tag.has_attr("rowspan")
            for tag in soup.find_all(["td", "th"])
        )
        if has_colspan_or_rowspan:
            # å¦‚æœå­˜åœ¨åˆå¹¶å•å…ƒæ ¼ï¼Œåˆ™è°ƒç”¨_process_table_elementå¤„ç†æ•´ä¸ªè¡¨æ ¼å…ƒç´ 
            # è¿”å›çš„æ˜¯ç®€åŒ–å±æ€§åçš„HTMLå­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯Markdown
            return self._process_table_element(el)
        else:
            # å¦‚æœæ²¡æœ‰åˆå¹¶å•å…ƒæ ¼ï¼Œåˆ™è°ƒç”¨çˆ¶ç±»çš„convert_tableæ–¹æ³•è¿›è¡Œæ ‡å‡†Markdownè½¬æ¢
            return super().convert_table(el, text, parent_tags)

    def convert_a(self, el, text, parent_tags):
        """
        è½¬æ¢<a>æ ‡ç­¾ï¼ˆé“¾æ¥ï¼‰ä¸ºMarkdownæ ¼å¼ã€‚
        å¦‚æœæä¾›äº†current_urlï¼Œåˆ™å°†é“¾æ¥hrefè½¬æ¢ä¸ºç»å¯¹è·¯å¾„ã€‚
        å¤„ç†è‡ªåŠ¨é“¾æ¥ï¼ˆautolinksï¼‰å’Œé»˜è®¤æ ‡é¢˜ï¼ˆdefault_titleï¼‰çš„é€‰é¡¹ã€‚

        :param el: BeautifulSoupçš„Tagå¯¹è±¡ï¼Œä»£è¡¨<a>å…ƒç´ ã€‚
        :param text: é“¾æ¥çš„æ˜¾ç¤ºæ–‡æœ¬ã€‚
        :param convert_as_inline: å¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºæ˜¯å¦åº”å°†æ­¤é“¾æ¥ä½œä¸ºå†…è”å…ƒç´ å¤„ç†ã€‚
        :return: Markdownæ ¼å¼çš„é“¾æ¥å­—ç¬¦ä¸²ï¼Œæˆ–è€…åœ¨ç‰¹å®šæƒ…å†µä¸‹è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
        """

        # ä½¿ç”¨chompå‡½æ•°å¤„ç†é“¾æ¥æ–‡æœ¬ï¼Œåˆ†ç¦»å‰å¯¼/å°¾éšç©ºæ ¼
        prefix, suffix, text = chomp(text)
        if not text:
            # å¦‚æœé“¾æ¥æ–‡æœ¬ä¸ºç©ºï¼ˆä¾‹å¦‚ç©ºçš„<a></a>æ ‡ç­¾ï¼‰ï¼Œåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
            return ""

        # è·å–é“¾æ¥çš„hrefå’Œtitleå±æ€§
        href_url = el.get("href")
        title_text = el.get("title")

        if self.current_url and href_url:
            # å¦‚æœéœ€è¦å°†é“¾æ¥hrefè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            # ä½¿ç”¨urljoinå°†href_urlï¼ˆå¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„ï¼‰ä¸current_urlåˆå¹¶ä¸ºç»å¯¹è·¯å¾„
            href_url = urljoin(self.current_url, href_url)

        # å¤„ç†Markdownifyçš„autolinksé€‰é¡¹ï¼šå¦‚æœé“¾æ¥æ–‡æœ¬å’Œhrefç›¸åŒï¼Œä¸”æ— æ ‡é¢˜ï¼Œåˆ™ä½¿ç”¨<href>æ ¼å¼
        if (
            self.options.get("autolinks", False)  # æ£€æŸ¥autolinksé€‰é¡¹æ˜¯å¦å­˜åœ¨ä¸”ä¸ºTrue
            and text.replace(r"\_", "_")
            == href_url  # æ–‡æœ¬ï¼ˆå¤„ç†è½¬ä¹‰çš„ä¸‹åˆ’çº¿åï¼‰ä¸hrefç›¸åŒ
            and not title_text  # æ²¡æœ‰titleå±æ€§
            and not self.options.get("default_title", False)
        ):  # default_titleé€‰é¡¹æœªå¼€å¯
            return f"<{href_url}>"  # è¿”å›è‡ªåŠ¨é“¾æ¥æ ¼å¼

        # å¤„ç†Markdownifyçš„default_titleé€‰é¡¹ï¼šå¦‚æœæ²¡æœ‰titleå±æ€§ï¼Œä½†å¼€å¯äº†default_titleï¼Œåˆ™ä½¿ç”¨hrefä½œä¸ºtitle
        if self.options.get("default_title", False) and not title_text and href_url:
            title_text = href_url

        # å¤„ç†é“¾æ¥æ ‡é¢˜ï¼Œå¦‚æœå­˜åœ¨ï¼Œåˆ™è¿›è¡Œè½¬ä¹‰å¹¶æ ¼å¼åŒ–
        if title_text:
            escaped_title = title_text.replace('"', r"\"")  # è½¬ä¹‰åŒå¼•å·
            title_part = f' "{escaped_title}"'  # æ ¼å¼åŒ–ä¸º "title"
        else:
            title_part = ""  # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œåˆ™ä¸ºç©º

        # è¿”å›æ ‡å‡†Markdownæ ¼å¼çš„é“¾æ¥ï¼š[text](href_url "title_text")
        # å¦‚æœhref_urlä¸ºç©ºæˆ–Noneï¼Œåˆ™åªè¿”å›å¤„ç†è¿‡çš„æ–‡æœ¬ï¼ˆprefix + text + suffixï¼‰
        return (
            f"{prefix}[{text}]({href_url}{title_part}){suffix}"
            if href_url
            else f"{prefix}{text}{suffix}"
        )

    # åŠ ç²—æ ‡ç­¾<b>çš„è½¬æ¢ï¼Œä½¿ç”¨markdownifyåº“æä¾›çš„abstract_inline_conversionè¾…åŠ©å‡½æ•°
    # self.options['strong_em_symbol'] é€šå¸¸æ˜¯ '*' æˆ– '_'
    # lambda self: 2 * self.options['strong_em_symbol'] è¡¨ç¤ºä½¿ç”¨ä¸¤ä¸ªç¬¦å·åŒ…è£¹æ–‡æœ¬ï¼Œä¾‹å¦‚ **text**
    convert_b = abstract_inline_conversion(
        lambda self: 2 * self.options.get("strong_em_symbol", "*")
    )

    # å¼ºè°ƒæ ‡ç­¾<em>æˆ–<i>çš„è½¬æ¢ï¼ŒåŒæ ·ä½¿ç”¨abstract_inline_conversion
    # lambda self: self.options['strong_em_symbol'] è¡¨ç¤ºä½¿ç”¨ä¸€ä¸ªç¬¦å·åŒ…è£¹æ–‡æœ¬ï¼Œä¾‹å¦‚ *text*
    convert_em = abstract_inline_conversion(
        lambda self: self.options.get("strong_em_symbol", "*")
    )
    convert_i = convert_em  # <i>æ ‡ç­¾é€šå¸¸ä¸<em>è¡Œä¸ºä¸€è‡´

    # åˆ é™¤çº¿æ ‡ç­¾<del>æˆ–<s>çš„è½¬æ¢
    convert_del = abstract_inline_conversion(lambda self: "~~")
    convert_s = convert_del  # <s>æ ‡ç­¾é€šå¸¸ä¸<del>è¡Œä¸ºä¸€è‡´

# åŒ¹é…Markdownå›¾ç‰‡è¯­æ³•çš„æ­£åˆ™è¡¨è¾¾å¼
IMG_TAG_RE = re.compile(r'!\[.*?\]\((https?://[^\)]+)\)', re.IGNORECASE)


def extract_img_urls(markdown: str) -> List[str]:
    """æå–Markdownä¸­æ‰€æœ‰è¿œç¨‹å›¾ç‰‡URLï¼ˆhttp/httpsï¼‰"""
    urls = IMG_TAG_RE.findall(markdown)
    return list(dict.fromkeys(urls))  # å»é‡


async def analyze_images_concurrently(
    img_urls: List[str], analyzer: AsyncImageAnalysis
) -> List[Dict[str, Any]]:
    tasks = [analyzer.analyze_image(image_url=url) for url in img_urls]
    return await asyncio.gather(*tasks)


def replace_img_tags_with_markdown(
    html: str, img_results: List[Dict[str, Any]], img_urls: List[str]
) -> str:
    """å°†imgæ ‡ç­¾æ›¿æ¢ä¸ºå¸¦AIæè¿°çš„Markdownå›¾ç‰‡è¯­æ³•"""

    def replacement(match):
        url = match.group(1)
        if url in img_urls:
            idx = img_urls.index(url)
            result = img_results[idx]
            if result and not result.get("error"):
                title = result.get("title", "å›¾ç‰‡")
                desc = result.get("description", "")
                md = f"![{title}]({url})"
                if desc:
                    md += "\n" + "\n".join(
                        f"> {line}" for line in desc.strip().splitlines()
                    )
                return md
        return match.group(0)

    return IMG_TAG_RE.sub(replacement, html)


def html2md_with_concurrent_image_analysis(
    html: str, analyzer: AsyncImageAnalysis
) -> str:
    """æ‰¹é‡å¹¶å‘åˆ†æå›¾ç‰‡å¹¶æ›¿æ¢ä¸ºå¸¦AIæè¿°çš„Markdownå›¾ç‰‡è¯­æ³•"""
    img_urls = extract_img_urls(html)
    if not img_urls:
        return html
    img_results = asyncio.run(analyze_images_concurrently(img_urls, analyzer))
    return replace_img_tags_with_markdown(html, img_results, img_urls)


async def analyze_images_from_html(html, provider="zhipu", max_concurrent=10):
    soup = BeautifulSoup(html, "html.parser")
    img_tags = soup.find_all("img")
    img_srcs = [img.get("src") for img in img_tags if img.get("src")]
    if not img_srcs:
        return {}
    # å»é‡
    img_srcs = list(dict.fromkeys(img_srcs))
    # æ„é€ åˆ†æè¾“å…¥
    image_sources = [{"image_url": src} for src in img_srcs]
    async with AsyncImageAnalysis(
        provider=provider, max_concurrent=max_concurrent
    ) as analyzer:
        results = await analyzer.analyze_multiple_images(image_sources)
    # ç»„è£… src -> {title, description}
    img_desc_map = {}
    for src, res in zip(img_srcs, results):
        if isinstance(res, dict):
            img_desc_map[src] = {
                "title": res.get("title", ""),
                "description": res.get("description", ""),
            }
    return img_desc_map


def convert_url_to_markdown(
    url: str,
    provider: str = "zhipu",
    api_key: str = None,
    base_url: str = None,
    vision_model: str = None, 
    max_concurrent: int = 10,
    analyze_images: bool = True,  
    add_frontmatter: bool = True,  
) -> Optional[str]:
    """
    è·å–ç½‘é¡µä¸»è¦å†…å®¹ï¼Œè½¬æ¢ä¸ºå¸¦YAML Frontmatterçš„Markdownå­—ç¬¦ä¸²ã€‚
    :param url: æ–‡ç« URL
    :param provider: å›¾ç‰‡åˆ†æAPIæä¾›å•†
    :param api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆçº§é«˜äºç¯å¢ƒå˜é‡ï¼‰
    :param base_url: APIåŸºç¡€URLï¼ˆå¯é€‰ï¼Œä¼˜å…ˆçº§é«˜äºç¯å¢ƒå˜é‡ï¼‰
    :param vision_model: è§†è§‰æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆçº§é«˜äºç¯å¢ƒå˜é‡å’Œé»˜è®¤å€¼ï¼‰
    :param max_concurrent: æœ€å¤§å›¾ç‰‡åˆ†æå¹¶å‘æ•°
    :param analyze_images: æ˜¯å¦å¼€å¯å›¾ç‰‡åˆ†æ
    :param add_frontmatter: æ˜¯å¦æ·»åŠ YAML frontmatterï¼ˆæ–°å¢ï¼‰
    :return: Markdownå­—ç¬¦ä¸²æˆ–None
    """
    print(f"ğŸš€ æ­£åœ¨å¤„ç† URL: {url}\n")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    try:
        resp = requests.get(url, headers=headers, timeout=20)
        resp.raise_for_status()
        
        # --- æ­¥éª¤ 1: ä½¿ç”¨trafilaturaæå–å†…å®¹ ---
        html_content = trafilatura.extract(
            resp.content,
            include_comments=False,
            include_tables=True,
            include_images=True,
            include_links=True,
        )
        json_output = trafilatura.extract(
            resp.content,
            output_format="json",
            include_comments=False,
            include_tables=True,
        )

        if not html_content and not json_output:
            print("âŒ æå–å†…å®¹å¤±è´¥ï¼Œé¡µé¢å¯èƒ½ä¸å…¼å®¹æˆ–æ— æ­£æ–‡ã€‚")
            return None

        metadata = {
            "title": "Untitled",
            "author": None,
            "date": None,
            "source": url,
        }
        data = {}
        if json_output:
            try:
                data = json.loads(json_output)
                metadata.update(
                    {
                        "title": data.get("title", "Untitled"),
                        "author": data.get("author"),
                        "date": data.get("date"),
                        "source": data.get("source") or url,
                    }
                )
            except json.JSONDecodeError:
                print("âŒ è§£ææå–çš„JSONæ•°æ®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å…ƒæ•°æ®ã€‚")

        # ä½¿ç”¨HTMLå†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•ä»JSONè·å–æ–‡æœ¬
        main_content = html_content
        if not main_content and json_output:
            try:
                data = json.loads(json_output)
                main_content = (
                    data.get("text", "")
                    or data.get("raw_text", "")
                    or data.get("content", "")
                )
            except json.JSONDecodeError:
                pass

        if not main_content:
            print("âŒ æœªèƒ½æå–åˆ°ä»»ä½•æ–‡æœ¬å†…å®¹")
            return None

        print(f"ğŸ“ æå–åˆ°çš„å†…å®¹é•¿åº¦: {len(main_content)} å­—ç¬¦")        # --- æ­¥éª¤ 3: å¤„ç†HTMLå†…å®¹å¹¶è½¬æ¢ä¸ºMarkdown ---
        if "<" in main_content and ">" in main_content:
            clean_html = main_content
        else:
            clean_html = f"<p>{main_content.replace(chr(10), '</p><p>')}</p>"
        
        # å…ˆè½¬æ¢ä¸ºåŸºç¡€Markdown
        converter = ImageDescMarkdownConverter(
            heading_style="ATX", wrap=True, wrap_width=80
        )
        markdown_body = converter.convert(clean_html)
        
        # --- æ­¥éª¤ 4: å¯¹Markdownä¸­çš„å›¾ç‰‡è¿›è¡Œåˆ†æå’Œæ›¿æ¢ ---
        if analyze_images:
            print(f"ğŸ” å¼€å§‹å›¾ç‰‡åˆ†æï¼Œprovider: {provider}")
            # ç”¨æ­£åˆ™ä»Markdownä¸­æå–å›¾ç‰‡URL
            img_urls = re.findall(r'!\[.*?\]\((https?://[^\)]+)\)', markdown_body)
            
            if img_urls:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

                async def analyze_with_params():
                    async with AsyncImageAnalysis(
                        provider=provider,
                        api_key=api_key,
                        base_url=base_url,
                        vision_model=vision_model,  # ä¼ é€’è§†è§‰æ¨¡å‹
                        max_concurrent=max_concurrent,
                    ) as analyzer:
                        img_srcs_unique = list(dict.fromkeys(img_urls))
                        image_sources = [{"image_url": src} for src in img_srcs_unique]
                        print(f"ğŸ”® å¼€å§‹åˆ†æ {len(img_srcs_unique)} ä¸ªå”¯ä¸€å›¾ç‰‡...")
                        results = await analyzer.analyze_multiple_images(image_sources)
                        print(f"ğŸ¯ åˆ†æç»“æœ: {results}")
                        return results

                results = loop.run_until_complete(analyze_with_params())
                  # æ›¿æ¢Markdownä¸­çš„å›¾ç‰‡
                img_srcs_unique = list(dict.fromkeys(img_urls))
                for i, img_url in enumerate(img_srcs_unique):
                    if i < len(results) and isinstance(results[i], dict) and not results[i].get("error"):
                        title = results[i].get("title", "å›¾ç‰‡")
                        desc = results[i].get("description", "")
                        
                        # æ„å»ºæ–°çš„å›¾ç‰‡Markdown
                        new_img_md = f"![{title}]({img_url})"
                        if desc:
                            new_img_md += "\n" + "\n".join(f"> {line}" for line in desc.strip().splitlines())
                        
                        # æ›¿æ¢åŸæœ‰çš„å›¾ç‰‡æ ‡è®°
                        old_pattern = rf'!\[.*?\]\({re.escape(img_url)}\)'
                        markdown_body = re.sub(old_pattern, new_img_md, markdown_body)
                        
                print("âœ… å›¾ç‰‡åˆ†æå’Œæ›¿æ¢å®Œæˆ")
        else:
            print("â­ï¸ è·³è¿‡å›¾ç‰‡åˆ†æ")

        # --- æ­¥éª¤ 5: å¤„ç†å…ƒæ•°æ® (å¯¹åº” dayjs) ---
        try:
            if metadata["date"]:
                parsed_date = parse(metadata["date"])
                # è½¬æ¢ä¸ºå¸¦æ—¶åŒºçš„æ ‡å‡†æ ¼å¼
                standard_date = parsed_date.astimezone(timezone.utc).strftime(
                    "%Y-%m-%d"
                )
                metadata["date"] = standard_date
            else:
                metadata["date"] = ""
        except (ValueError, TypeError):
            metadata["date"] = ""  # è§£æå¤±è´¥åˆ™ç•™ç©º

        # --- æ­¥éª¤ 6: ç»„åˆ YAML Frontmatter å’Œ Markdown æ­£æ–‡ ---
        if add_frontmatter:
            yaml_frontmatter = "---\n"
            yaml_frontmatter += f"title: \"{metadata['title']}\"\n"
            if metadata["author"]:
                yaml_frontmatter += f"author: \"{metadata['author']}\"\n"
            if metadata["date"]:
                yaml_frontmatter += f"date: {metadata['date']}\n"
            yaml_frontmatter += f"source: <{metadata['source']}>\n"
            yaml_frontmatter += "---\n\n"
        else:
            yaml_frontmatter = ""

        final_content = yaml_frontmatter + markdown_body

        return final_content

    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return None





# --- ä¸»ç¨‹åºæ‰§è¡ŒåŒºåŸŸ ---
if __name__ == "__main__":
    # æ‚¨å¯ä»¥æ›¿æ¢æˆä»»ä½•æƒ³è¦æµ‹è¯•çš„æ–‡ç« é“¾æ¥
    # ç¤ºä¾‹1: æŠ€æœ¯åšå®¢
    test_url = "https://www.ruanyifeng.com/blog/2024/05/weekly-issue-299.html"

    # ç¤ºä¾‹2: æ–°é—»æ–‡ç« 
    # test_url = "https://www.theverge.com/2024/5/13/24155243/openai-gpt-4o-announcements-google-io"

    # ä».envæ–‡ä»¶è¯»å–ZHIPUçš„API KEYå’ŒBASE_URL
    from dotenv import load_dotenv

    load_dotenv()
    import os

    zhipu_api_key = os.getenv("ZHIPU_API_KEY")
    zhipu_base_url = os.getenv("ZHIPU_BASE_URL")
    zhipu_model = os.getenv("ZHIPU_MODEL")

    # è°ƒç”¨å‡½æ•°å¹¶è·å–è¿”å›çš„Markdownå­—ç¬¦ä¸²ï¼Œä¼ å…¥keyå’Œbase_url
    markdown_output = convert_url_to_markdown(
        test_url,
        provider="zhipu",
        api_key=zhipu_api_key,
        base_url=zhipu_base_url,
        analyze_images=True  # æ˜¾å¼æ§åˆ¶æ˜¯å¦å¼€å¯å›¾ç‰‡åˆ†æ
    )

    # å¦‚æœæˆåŠŸè·å–ï¼Œåˆ™ä¿å­˜åˆ°æ–‡ä»¶å¹¶æ‰“å°åˆ°æ§åˆ¶å°
    if markdown_output:
        # ä¿å­˜åˆ° test.md æ–‡ä»¶
        output_file = "test.md"
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(markdown_output)
            print(f"âœ… å†…å®¹å·²æˆåŠŸä¿å­˜åˆ° {output_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")

        print("---------- MARKDOWN è¾“å‡ºå¼€å§‹ ----------\n")
        print(markdown_output)
        print("\n----------- MARKDOWN è¾“å‡ºç»“æŸ -----------")
