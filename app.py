import streamlit as st
from duckduckgo_search import DDGS
from html2md import convert_url_to_markdown
import os
import openai
import requests
import asyncio
import concurrent.futures


st.set_page_config(page_title="è”ç½‘æœç´¢å¯¹è¯ç³»ç»Ÿ", layout="wide")
st.title("ğŸ” è”ç½‘æœç´¢å¯¹è¯ç³»ç»Ÿ ")

# Sidebar options
st.sidebar.title("é…ç½®é€‰é¡¹")
analyze_images_enabled = st.sidebar.checkbox("å¼€å¯å›¾ç‰‡åˆ†æ", value=False)
if st.sidebar.button("æ¸…ç©ºå¯¹è¯è®°å½•", use_container_width=True):
    st.session_state["history"] = []
    st.session_state["search_results"] = []


def search_duckduckgo(query, max_results=3, proxies=None, user_agent=None):
    ddgs = DDGS()
    # é€šè¿‡requestsè‡ªå®šä¹‰user-agentå’Œä»£ç†
    headers = {
        "User-Agent": user_agent
        or "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    results = []
    for r in ddgs.text(query, max_results=max_results):
        # è¿™é‡Œå¯ä»¥ç”¨requests.headæµ‹è¯•ä»£ç†å’Œuser-agent
        try:
            requests.head(
                r.get("href") or r.get("url"),
                headers=headers,
                proxies=proxies,
                timeout=3,
            )
        except Exception:
            pass
        results.append(r)
    return results


def fetch_and_convert(url, add_frontmatter=True, analyze_images=False):
    return convert_url_to_markdown(
        url,
        provider="guiji",
        api_key=os.getenv("GUIJI_API_KEY"),
        base_url=os.getenv("GUIJI_BASE_URL"),
        analyze_images=analyze_images,
        add_frontmatter=add_frontmatter,
    )


async def fetch_and_convert_async(url, add_frontmatter=True, analyze_images=False, session=None):
    loop = asyncio.get_event_loop()
    try:
        return await loop.run_in_executor(
            None, lambda: fetch_and_convert(url, add_frontmatter=add_frontmatter, analyze_images=analyze_images)
        )
    except Exception:
        return None


def call_guiji_rag_model(query, answer_blocks):
    guiji_api_key = os.getenv("GUIJI_API_KEY")
    guiji_base_url = os.getenv("GUIJI_BASE_URL")
    guiji_model = os.getenv("GUIJI_TEXT_MODEL")
    prompt = f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\nå‚è€ƒå†…å®¹ï¼ˆå¯åŒ…å«å›¾ç‰‡ï¼‰ï¼š\n" + "\n\n".join(
        answer_blocks
    )
    openai.api_key = guiji_api_key
    openai.base_url = guiji_base_url
    response = openai.chat.completions.create(
        model=guiji_model,
        messages=[
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ å¯ä»¥å‚è€ƒä¸‹æ–¹æä¾›çš„å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡æœ¬å’Œå›¾ç‰‡ï¼‰ï¼Œç»“åˆè‡ªå·±çš„çŸ¥è¯†ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚ä½ å¯ä»¥å¼•ç”¨å‚è€ƒå†…å®¹ä¸­çš„å›¾ç‰‡ï¼ˆä»¥Markdownæ ¼å¼è¾“å‡ºï¼‰æ¥å¢å¼ºä½ çš„å›ç­”æ•ˆæœã€‚å›ç­”æ—¶è¯·å°½é‡å¼•ç”¨æœ‰ç”¨çš„å†…å®¹å’Œå›¾ç‰‡ï¼Œæå‡äº¤äº’ä½“éªŒã€‚",
            },
            {"role": "user", "content": prompt},
        ],
        max_tokens=4096
    )
    return response.choices[0].message.content


def call_guiji_rag_model_stream(query, answer_blocks, _, chat_history=None):
    client = openai.OpenAI(
        api_key=os.getenv("GUIJI_API_KEY"), base_url=os.getenv("GUIJI_BASE_URL")
    )
    guiji_model = os.getenv("GUIJI_TEXT_MODEL")
    # æ‹¼æ¥å‚è€ƒå†…å®¹ï¼Œæ¯ç¯‡ç”¨ã€ç¬¬nç¯‡å‚è€ƒæ–‡ç« å¼€å§‹ã€‘å’Œã€ç¬¬nç¯‡å‚è€ƒæ–‡ç« ç»“æŸã€‘åŒ…å›´ï¼Œå‰ç¼€åŠ [ç¼–å·]
    ref_content = ""
    for idx, md in enumerate(answer_blocks):
        ref_content += (
            f"ã€ç¬¬{idx+1}ç¯‡å‚è€ƒæ–‡ç« å¼€å§‹ã€‘\n[${idx+1}]\n"
            + md
            + f"\nã€ç¬¬{idx+1}ç¯‡å‚è€ƒæ–‡ç« ç»“æŸã€‘\n"
        )
    prompt = f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\nå‚è€ƒå†…å®¹å¦‚ä¸‹ï¼ˆå¯åŒ…å«å›¾ç‰‡ï¼‰ï¼š\n{ref_content}"
    sys_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ åªèƒ½å‚è€ƒä¸‹æ–¹æä¾›çš„å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡æœ¬å’Œå›¾ç‰‡ï¼‰ï¼Œç»“åˆè‡ªå·±çš„çŸ¥è¯†ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
        "å›ç­”æ­£æ–‡ä¸­å¦‚æœ‰å¼•ç”¨å†…å®¹ï¼Œè¯·ç”¨[1][2]ç­‰è§’æ ‡æ ‡æ³¨ã€‚"
        "å›ç­”æœ€åè¯·ä»¥'å‚è€ƒæ–‡ç« ï¼š'çš„å½¢å¼ï¼Œåˆ—å‡ºä½ ç”¨åˆ°çš„æ–‡ç« ç¼–å·å’Œå¯¹åº”é“¾æ¥ï¼Œæ ¼å¼å¦‚ï¼š\nå‚è€ƒæ–‡ç« ï¼š\n[1] é“¾æ¥1\n[2] é“¾æ¥2ã€‚"
        "å¦‚æœæ²¡æœ‰å‚è€ƒä»»ä½•æ–‡ç« ï¼Œå¯ä»¥çœç•¥è¯¥éƒ¨åˆ†ã€‚"
        "è¯·ä¸¥æ ¼ä¸è¦å°†ã€ç¬¬nç¯‡å‚è€ƒæ–‡ç« å¼€å§‹ã€‘å’Œã€ç¬¬nç¯‡å‚è€ƒæ–‡ç« ç»“æŸã€‘å¤–çš„å†…å®¹å½“ä½œå‚è€ƒèµ„æ–™ã€‚"
        "å¦‚æœå‚è€ƒå†…å®¹é‡Œé¢æ²¡æœ‰æ­£ç¡®ç­”æ¡ˆï¼Œé‚£ä¹ˆå°±å›ç­”ç”¨æˆ·æˆ‘ä¸çŸ¥é“å³å¯ã€‚"
        "ä¸è¦è¾“å‡ºä»»ä½•å‚è€ƒå†…å®¹ä»¥å¤–çš„å†…å®¹ã€‚"
    )
    messages = [
        {"role": "system", "content": sys_prompt},
        {"role": "user", "content": prompt},
    ]
    if chat_history:
        # å°†å†å²è®°å½•æ‹¼æ¥åˆ°æ¶ˆæ¯ä¸­
        for chat in chat_history:
            messages.insert(-1, chat)  # åœ¨å€’æ•°ç¬¬äºŒä¸ªä½ç½®æ’å…¥ï¼Œä¿æŒç”¨æˆ·é—®é¢˜åœ¨æœ€å

    response = client.chat.completions.create(
        model=guiji_model, messages=messages, stream=True, max_tokens=4096
    )
    return response


def process_search_and_content(query, max_results=10, proxies=None, user_agent=None, analyze_images=False):
    """å¹¶å‘æŠ“å–å†…å®¹ï¼Œæ— æ³•è·å–çš„ç›´æ¥ç”¨æœç´¢bodyã€‚"""
    results = search_duckduckgo(
        query, max_results=max_results, proxies=proxies, user_agent=user_agent
    )
    search_summaries = []
    urls = []
    bodies = []
    for idx, r in enumerate(results):
        url = r.get("href") or r.get("url")
        title = r.get("title")
        snippet = r.get("body") or r.get("snippet")
        search_summaries.append((title, url, snippet))
        urls.append(url)
        bodies.append(snippet)

    async def batch_fetch():
        tasks = [fetch_and_convert_async(url, add_frontmatter=True, analyze_images=analyze_images) if url else None for url in urls]
        return await asyncio.gather(*tasks)

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    md_results = loop.run_until_complete(batch_fetch())
    answer_blocks = []
    for idx, md in enumerate(md_results):
        if md and md.strip():
            answer_blocks.append(md)
        else:
            # ç”¨åŸå§‹bodyä½œä¸ºå…œåº•å†…å®¹
            answer_blocks.append(bodies[idx] or "")
    return search_summaries, answer_blocks


# æœç´¢è¾“å…¥æ¡†
query = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜æˆ–å…³é”®è¯ï¼š", "OpenAI GPT-4o æœ‰å“ªäº›æ–°ç‰¹æ€§ï¼Ÿ")

# èŠå¤©å†å²ä¸è¾“å…¥æ¡†ï¼ˆä½¿ç”¨st.chat_messageå’Œst.chat_inputå®ç°åŸç”ŸChatGPTä½“éªŒï¼‰
if "history" not in st.session_state:
    st.session_state["history"] = []
if "search_results" not in st.session_state:
    st.session_state["search_results"] = []

for msg in st.session_state["history"]:
    with st.chat_message("user" if msg["role"] == "user" else "assistant"):
        st.markdown(msg["content"])

user_input = st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜...")

if user_input and user_input.strip():
    st.session_state["history"].append({"role": "user", "content": user_input.strip()})
    proxies = None  # ä¾‹å¦‚{"http": "http://127.0.0.1:7890", "https": "http://127.0.0.1:7890"}
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    with st.status("æ­£åœ¨è”ç½‘æœç´¢å’Œå¤„ç†å†…å®¹ï¼Œè¯·ç¨å€™...", expanded=True) as status:
        try:
            status.write("ğŸ” æ­£åœ¨è¿›è¡Œ DuckDuckGo æœç´¢...")
            search_summaries, answer_blocks = process_search_and_content(
                user_input, proxies=proxies, user_agent=user_agent, analyze_images=analyze_images_enabled
            )
            st.session_state["search_results"] = search_summaries           
            status.write(f"âœ… æœç´¢å®Œæˆï¼Œå…±{len(search_summaries)}æ¡ç»“æœã€‚")
            if answer_blocks:
                status.write("ğŸ¤– æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹æµå¼ç”Ÿæˆå›ç­”...")
                chat_history = st.session_state["history"][-5:]
                response = call_guiji_rag_model_stream(
                    user_input, answer_blocks, None, chat_history
                )
                full_answer = ""
                with st.chat_message("assistant"):
                    stream_placeholder = st.empty()
                    for chunk in response:
                        delta = (
                            chunk.choices[0].delta.content
                            if chunk.choices[0].delta
                            else ""
                        )
                        if delta:
                            full_answer += delta
                            stream_placeholder.markdown(full_answer)
                st.session_state["history"].append(
                    {"role": "assistant", "content": full_answer}
                )
                status.write("âœ… å›ç­”ç”Ÿæˆå®Œæ¯•ï¼")
        except Exception as e:
            st.error(f"æœç´¢æˆ–å†…å®¹æŠ“å–å¤±è´¥: {e}")
            status.write("âŒ æœç´¢æˆ–å†…å®¹æŠ“å–å¤±è´¥")    # åœ¨statuså®¹å™¨å¤–éƒ¨æ˜¾ç¤ºæœç´¢ç»“æœ
if st.session_state.get("search_results"):
    with st.expander("ğŸ” å±•å¼€/æ”¶èµ·å…¨éƒ¨æœç´¢ç»“æœ", expanded=False):
        st.write("### æœç´¢ç»“æœ")
        for idx, (title, url, snippet) in enumerate(st.session_state["search_results"]):
            st.markdown(f"**[{idx+1}] [{title}]({url})**")
            st.markdown(f"**é“¾æ¥ï¼š** [{url}]({url})")
            # if snippet:
            #     st.markdown(f"**æ‘˜è¦ï¼š** {snippet}")
